<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>1009-crawler-merged_ProgramNLP.RedditScraper API documentation</title>
<meta name="description" content="RedditScraper.py contains subclass of Scraper, RedditScraper and its attributes plus functions" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>1009-crawler-merged_ProgramNLP.RedditScraper</code></h1>
</header>
<section id="section-intro">
<p>RedditScraper.py contains subclass of Scraper, RedditScraper and its attributes plus functions</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;RedditScraper.py contains subclass of Scraper, RedditScraper and its attributes plus functions &#34;&#34;&#34;

from Scraper import Scraper
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.common.keys import Keys
from time import sleep
from selenium.common.exceptions import *
import CustomExceptions as ex

class RedditScraper(Scraper):
    &#34;&#34;&#34;
    The RedditScraper object scrapes all comments of reddit posts until the desired sample size specified in the main program is reached.

    Args:
        Scraper (class): inherit general attributes and functions needed for scraping from superclass
        query (str): Used as part of fully qualified domain name to to directly open Reddit to display search results.
        sample_size (int): Controls number of tweets/posts that will be scraped from the social media
    
    Attributes/Class variables:
    
        url (str): base url of Reddit. Used as first part of fully qualified domain name
        time_span (str): Last part of Reddit fully qualified domain name
        post_attr (str): CSS selector of element used to store comments of scraped Reddit posts
        comment_attr (str): Class name of post hyperlink to click on Reddit search result page

        reddit_comment (list): List to store formatted comments to write into csv file
        reddit_post (set): Self-defined set subclass to check for duplicate posts
        file_name (str): Name of csv file to write scraped data to. Derived from search term/query
        fully_qualified_domain (str): Full URL used to directly open Reddit with search results
        browser (webdriver): Starts a selenium webdriver instance which open Reddit search results in chrome
    &#34;&#34;&#34;
    url = &#34;https://reddit.com/search?q=&#34;
    time_span = &#34;&amp;t=month&#34;
    post_attr = &#34;_eYtD2XCVieq6emjKBH3m&#34;
    comment_attr = {&#34;class&#34;: &#34;_1qeIAgB0cPwnLhDF9XSiJM&#34;}

    def __init__(self, query, sample_size):
        super().__init__(query, sample_size)
        self.__reddit_comment = []
        self.__reddit_post = ex.Set_duplicate_detector()
        self.__file_name = &#34;{}_reddit.csv&#34;.format(self.query)
        self.__fully_qualified_domain = self.url + self.query + self.time_span
        self.__browser = self.initialise_webdriver(self.fully_qualified_domain)

    @property
    def reddit_comment(self):
        return self.__reddit_comment

    @reddit_comment.setter
    def reddit_comment(self, reddit_comment):
        self.__reddit_comment = reddit_comment

    @property
    def reddit_post(self):
        return self.__reddit_post

    @reddit_post.setter
    def reddit_post(self, reddit_post):
        self.__reddit_post = reddit_post

    @property
    def file_name(self):
        return self.__file_name

    @file_name.setter
    def file_name(self, file_name):
        self.__file_name = file_name

    @property
    def fully_qualified_domain(self):
        return self.__fully_qualified_domain

    @fully_qualified_domain.setter
    def fully_qualified_domain(self, fully_qualified_domain):
        self.__fully_qualified_domain = fully_qualified_domain

    @property
    def browser(self):
        return self.__browser

    @browser.setter
    def browser(self, browser):
        self.__browser = browser

    def scrape(self):
        &#34;&#34;&#34;
        Main method that scrapes data from Reddit comments from posts displayed as search results

        Prepares scraped tweets to be analyzed by TextBlob in the VaccinePolarity class
        by appending each tweet to reddit_comment list

        Scraping will continue until sample_size = 0
        &#34;&#34;&#34;
        print(&#34;\n********************BEGIN REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))
        posts = self.browser.find_elements_by_class_name(RedditScraper.post_attr)
        if len(posts) == 0:
            raise ex.NoElementFound(RedditScraper.post_attr)
        for post in posts:
            print(&#34;SCRAPING POST: {}&#34;.format(post.text))
            try:
                self.reddit_post.add(post)
            except ex.DuplicateEntryError:
                print(&#34;Current post has already been scraped, skipping post&#34;)
                continue
            try:
                post.click()
            except ElementClickInterceptedException as intercept:
                print(intercept.msg)
                self.browser.execute_script(&#34;arguments[0].scrollIntoView({block: &#39;center&#39;});&#34;, post)
                sleep(0.5)
                post.click()
            except ElementNotInteractableException as interact:
                print(interact.msg)
                continue
            sleep(4)
            soup = BeautifulSoup(self.browser.page_source, &#39;html.parser&#39;)
            comment_counter = 0
            comments = soup.find_all(&#34;p&#34;, attrs=RedditScraper.comment_attr)
            if len(comments) == 0:
                raise ex.NoElementFound(RedditScraper.comment_attr)
            for comment in comments:
                if self.sample_size == 0 or comment_counter == 100:
                    break
                else:
                    formatted_comment = comment.get_text().replace(&#34;\n&#34;, &#34; &#34;)
                    self.reddit_comment.append(formatted_comment)
                    self.sample_size -= 1
                    comment_counter += 1
                    print(&#34;Comment {}: [{}]&#34;.format(self.sample_size + 1, formatted_comment))
            print(
                &#34;----------sample size remaining:{} | comments taken from post:{}----------&#34;.format(self.sample_size,
                                                                                                    comment_counter))
            webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
            webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
            sleep(0.5)
            if self.sample_size == 0:
                break
        if self.sample_size &gt; 0:
            print(&#34;-SCROLLING-\n&#34; * 10)
            self.last_position, self.end_of_scroll_region = self.scroll_down_page(self.browser, self.last_position)
            if self.end_of_scroll_region:
                print(&#34;UNABLE TO SCROLL FURTHER; INPUT SAMPLE SIZE HAS NOT BEEN MET&#34;)
                print(&#34;PLEASE RESTART PROGRAM AND TRY AGAIN&#34;)
                exit()
            else:
                self.scrape()
        print(&#34;\n********************END REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper"><code class="flex name class">
<span>class <span class="ident">RedditScraper</span></span>
<span>(</span><span>query, sample_size)</span>
</code></dt>
<dd>
<div class="desc"><p>The RedditScraper object scrapes all comments of reddit posts until the desired sample size specified in the main program is reached.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>Scraper</code></strong> :&ensp;<code>class</code></dt>
<dd>inherit general attributes and functions needed for scraping from superclass</dd>
<dt><strong><code>query</code></strong> :&ensp;<code>str</code></dt>
<dd>Used as part of fully qualified domain name to to directly open Reddit to display search results.</dd>
<dt><strong><code>sample_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Controls number of tweets/posts that will be scraped from the social media</dd>
</dl>
<p>Attributes/Class variables:</p>
<pre><code>url (str): base url of Reddit. Used as first part of fully qualified domain name
time_span (str): Last part of Reddit fully qualified domain name
post_attr (str): CSS selector of element used to store comments of scraped Reddit posts
comment_attr (str): Class name of post hyperlink to click on Reddit search result page

reddit_comment (list): List to store formatted comments to write into csv file
reddit_post (set): Self-defined set subclass to check for duplicate posts
file_name (str): Name of csv file to write scraped data to. Derived from search term/query
fully_qualified_domain (str): Full URL used to directly open Reddit with search results
browser (webdriver): Starts a selenium webdriver instance which open Reddit search results in chrome
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RedditScraper(Scraper):
    &#34;&#34;&#34;
    The RedditScraper object scrapes all comments of reddit posts until the desired sample size specified in the main program is reached.

    Args:
        Scraper (class): inherit general attributes and functions needed for scraping from superclass
        query (str): Used as part of fully qualified domain name to to directly open Reddit to display search results.
        sample_size (int): Controls number of tweets/posts that will be scraped from the social media
    
    Attributes/Class variables:
    
        url (str): base url of Reddit. Used as first part of fully qualified domain name
        time_span (str): Last part of Reddit fully qualified domain name
        post_attr (str): CSS selector of element used to store comments of scraped Reddit posts
        comment_attr (str): Class name of post hyperlink to click on Reddit search result page

        reddit_comment (list): List to store formatted comments to write into csv file
        reddit_post (set): Self-defined set subclass to check for duplicate posts
        file_name (str): Name of csv file to write scraped data to. Derived from search term/query
        fully_qualified_domain (str): Full URL used to directly open Reddit with search results
        browser (webdriver): Starts a selenium webdriver instance which open Reddit search results in chrome
    &#34;&#34;&#34;
    url = &#34;https://reddit.com/search?q=&#34;
    time_span = &#34;&amp;t=month&#34;
    post_attr = &#34;_eYtD2XCVieq6emjKBH3m&#34;
    comment_attr = {&#34;class&#34;: &#34;_1qeIAgB0cPwnLhDF9XSiJM&#34;}

    def __init__(self, query, sample_size):
        super().__init__(query, sample_size)
        self.__reddit_comment = []
        self.__reddit_post = ex.Set_duplicate_detector()
        self.__file_name = &#34;{}_reddit.csv&#34;.format(self.query)
        self.__fully_qualified_domain = self.url + self.query + self.time_span
        self.__browser = self.initialise_webdriver(self.fully_qualified_domain)

    @property
    def reddit_comment(self):
        return self.__reddit_comment

    @reddit_comment.setter
    def reddit_comment(self, reddit_comment):
        self.__reddit_comment = reddit_comment

    @property
    def reddit_post(self):
        return self.__reddit_post

    @reddit_post.setter
    def reddit_post(self, reddit_post):
        self.__reddit_post = reddit_post

    @property
    def file_name(self):
        return self.__file_name

    @file_name.setter
    def file_name(self, file_name):
        self.__file_name = file_name

    @property
    def fully_qualified_domain(self):
        return self.__fully_qualified_domain

    @fully_qualified_domain.setter
    def fully_qualified_domain(self, fully_qualified_domain):
        self.__fully_qualified_domain = fully_qualified_domain

    @property
    def browser(self):
        return self.__browser

    @browser.setter
    def browser(self, browser):
        self.__browser = browser

    def scrape(self):
        &#34;&#34;&#34;
        Main method that scrapes data from Reddit comments from posts displayed as search results

        Prepares scraped tweets to be analyzed by TextBlob in the VaccinePolarity class
        by appending each tweet to reddit_comment list

        Scraping will continue until sample_size = 0
        &#34;&#34;&#34;
        print(&#34;\n********************BEGIN REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))
        posts = self.browser.find_elements_by_class_name(RedditScraper.post_attr)
        if len(posts) == 0:
            raise ex.NoElementFound(RedditScraper.post_attr)
        for post in posts:
            print(&#34;SCRAPING POST: {}&#34;.format(post.text))
            try:
                self.reddit_post.add(post)
            except ex.DuplicateEntryError:
                print(&#34;Current post has already been scraped, skipping post&#34;)
                continue
            try:
                post.click()
            except ElementClickInterceptedException as intercept:
                print(intercept.msg)
                self.browser.execute_script(&#34;arguments[0].scrollIntoView({block: &#39;center&#39;});&#34;, post)
                sleep(0.5)
                post.click()
            except ElementNotInteractableException as interact:
                print(interact.msg)
                continue
            sleep(4)
            soup = BeautifulSoup(self.browser.page_source, &#39;html.parser&#39;)
            comment_counter = 0
            comments = soup.find_all(&#34;p&#34;, attrs=RedditScraper.comment_attr)
            if len(comments) == 0:
                raise ex.NoElementFound(RedditScraper.comment_attr)
            for comment in comments:
                if self.sample_size == 0 or comment_counter == 100:
                    break
                else:
                    formatted_comment = comment.get_text().replace(&#34;\n&#34;, &#34; &#34;)
                    self.reddit_comment.append(formatted_comment)
                    self.sample_size -= 1
                    comment_counter += 1
                    print(&#34;Comment {}: [{}]&#34;.format(self.sample_size + 1, formatted_comment))
            print(
                &#34;----------sample size remaining:{} | comments taken from post:{}----------&#34;.format(self.sample_size,
                                                                                                    comment_counter))
            webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
            webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
            sleep(0.5)
            if self.sample_size == 0:
                break
        if self.sample_size &gt; 0:
            print(&#34;-SCROLLING-\n&#34; * 10)
            self.last_position, self.end_of_scroll_region = self.scroll_down_page(self.browser, self.last_position)
            if self.end_of_scroll_region:
                print(&#34;UNABLE TO SCROLL FURTHER; INPUT SAMPLE SIZE HAS NOT BEEN MET&#34;)
                print(&#34;PLEASE RESTART PROGRAM AND TRY AGAIN&#34;)
                exit()
            else:
                self.scrape()
        print(&#34;\n********************END REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>Scraper.Scraper</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.comment_attr"><code class="name">var <span class="ident">comment_attr</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.post_attr"><code class="name">var <span class="ident">post_attr</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.time_span"><code class="name">var <span class="ident">time_span</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.url"><code class="name">var <span class="ident">url</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.browser"><code class="name">var <span class="ident">browser</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def browser(self):
    return self.__browser</code></pre>
</details>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.file_name"><code class="name">var <span class="ident">file_name</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def file_name(self):
    return self.__file_name</code></pre>
</details>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.fully_qualified_domain"><code class="name">var <span class="ident">fully_qualified_domain</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def fully_qualified_domain(self):
    return self.__fully_qualified_domain</code></pre>
</details>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_comment"><code class="name">var <span class="ident">reddit_comment</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reddit_comment(self):
    return self.__reddit_comment</code></pre>
</details>
</dd>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_post"><code class="name">var <span class="ident">reddit_post</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reddit_post(self):
    return self.__reddit_post</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.scrape"><code class="name flex">
<span>def <span class="ident">scrape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Main method that scrapes data from Reddit comments from posts displayed as search results</p>
<p>Prepares scraped tweets to be analyzed by TextBlob in the VaccinePolarity class
by appending each tweet to reddit_comment list</p>
<p>Scraping will continue until sample_size = 0</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def scrape(self):
    &#34;&#34;&#34;
    Main method that scrapes data from Reddit comments from posts displayed as search results

    Prepares scraped tweets to be analyzed by TextBlob in the VaccinePolarity class
    by appending each tweet to reddit_comment list

    Scraping will continue until sample_size = 0
    &#34;&#34;&#34;
    print(&#34;\n********************BEGIN REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))
    posts = self.browser.find_elements_by_class_name(RedditScraper.post_attr)
    if len(posts) == 0:
        raise ex.NoElementFound(RedditScraper.post_attr)
    for post in posts:
        print(&#34;SCRAPING POST: {}&#34;.format(post.text))
        try:
            self.reddit_post.add(post)
        except ex.DuplicateEntryError:
            print(&#34;Current post has already been scraped, skipping post&#34;)
            continue
        try:
            post.click()
        except ElementClickInterceptedException as intercept:
            print(intercept.msg)
            self.browser.execute_script(&#34;arguments[0].scrollIntoView({block: &#39;center&#39;});&#34;, post)
            sleep(0.5)
            post.click()
        except ElementNotInteractableException as interact:
            print(interact.msg)
            continue
        sleep(4)
        soup = BeautifulSoup(self.browser.page_source, &#39;html.parser&#39;)
        comment_counter = 0
        comments = soup.find_all(&#34;p&#34;, attrs=RedditScraper.comment_attr)
        if len(comments) == 0:
            raise ex.NoElementFound(RedditScraper.comment_attr)
        for comment in comments:
            if self.sample_size == 0 or comment_counter == 100:
                break
            else:
                formatted_comment = comment.get_text().replace(&#34;\n&#34;, &#34; &#34;)
                self.reddit_comment.append(formatted_comment)
                self.sample_size -= 1
                comment_counter += 1
                print(&#34;Comment {}: [{}]&#34;.format(self.sample_size + 1, formatted_comment))
        print(
            &#34;----------sample size remaining:{} | comments taken from post:{}----------&#34;.format(self.sample_size,
                                                                                                comment_counter))
        webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
        webdriver.ActionChains(self.browser).send_keys(Keys.ESCAPE).perform()
        sleep(0.5)
        if self.sample_size == 0:
            break
    if self.sample_size &gt; 0:
        print(&#34;-SCROLLING-\n&#34; * 10)
        self.last_position, self.end_of_scroll_region = self.scroll_down_page(self.browser, self.last_position)
        if self.end_of_scroll_region:
            print(&#34;UNABLE TO SCROLL FURTHER; INPUT SAMPLE SIZE HAS NOT BEEN MET&#34;)
            print(&#34;PLEASE RESTART PROGRAM AND TRY AGAIN&#34;)
            exit()
        else:
            self.scrape()
    print(&#34;\n********************END REDDIT SCRAPE FOR QUERY: &lt;{}&gt;********************\n&#34;.format(self.query))</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="1009-crawler-merged_ProgramNLP" href="index.html">1009-crawler-merged_ProgramNLP</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper">RedditScraper</a></code></h4>
<ul class="">
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.browser" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.browser">browser</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.comment_attr" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.comment_attr">comment_attr</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.file_name" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.file_name">file_name</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.fully_qualified_domain" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.fully_qualified_domain">fully_qualified_domain</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.post_attr" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.post_attr">post_attr</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_comment" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_comment">reddit_comment</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_post" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.reddit_post">reddit_post</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.scrape" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.scrape">scrape</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.time_span" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.time_span">time_span</a></code></li>
<li><code><a title="1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.url" href="#1009-crawler-merged_ProgramNLP.RedditScraper.RedditScraper.url">url</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>